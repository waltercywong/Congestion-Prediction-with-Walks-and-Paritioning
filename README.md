# Congestion Prediction in Chip Design with Walks and Paritioning

Chip design complexity has grown exponentially, leading to significant challenges in predicting and reducing wire congestion. Congestion occurs when excessive wire routing is concentrated in a constrained physical area, which degrades performance by causing overheating, signal interference, and excessive power usage.

This project explores congestion prediction in chip design by utilizing graph-based learning and partitioning techniques. We extend the work of the DE-HNN model by introducing dummy nodes to enhance long-range message passing and applying domain-based partitioning. These modifications aim to refine congestion-aware design decisions, optimizing chip layouts for better efficiency and performance. 

The repository includes notebooks and Python scripts for data analysis, model training, experimentation, and results of our tests. We evaluate these approaches against the original DE-HNN model, demonstrating their potential to improve congestion prediction in the early stages of chip design.

- Original DE-HNN repository: [Github](https://github.com/TILOS-AI-Institute/DEHNN) 
- Paper: [arXiv](https://arxiv.org/pdf/2404.00477) (Luo et al. (2024))

## File Structure

- `data`: contains all data generated by scripts and notebooks in src
- `src`: contains all the python scripts and exploratory notebooks
   - `scripts`: contains scripts that can be run from the repository that either manipulate data or perform analysis
      - `valid_pairs.py`: script for creating valid_pairs.csv
      - `random_walk_rep.py`: modifies `pyg_data.pkl` in the processed data to connect source-destination pairs from the results of random 
        walks. Saves it to `pyg_data_modified.pkl`
      - `analyze_pairs.py`: performs surface level analysis and visualizations
         on the `valid_pairs.csv` data
      - `analyze_pairs_detailed.py`: performs more in depth analysis and 
       visualizations on the `valid_pairs.csv` data
   - `notebooks`: contains all notebooks used for exploratory analysis
      - `data_exploration.ipynb`: contains exploratory analysis of DEHNN 
        datasets and model architecture
   - `pyg_dataset.py`: 
   - `train_all_cross.py`:
- `results`:
     - Baseline: contains DEHNN baseline model implementation results
     - Louvain: contains Louvain-based model implementation results
     - RandomWalk: contains RandomWalk model implementation results
     - Connections: contains Connections model implementation results
     - KMeans: contains KMeans clustering model implementation results
     - BalancedKMeans: contains Balanced KMeans model implementation results
     - XGBoost: contains XGBoost model implementation results
     - WeightedWalk: contains Weighted Walk model implementation results
     - XGBoostLimited: contains Limited XGBoost model implementation results

Each implementation in the `results` folder contains:

1. Training loss visualization (.png) showing RMSE for train, validation, and test results over 100 epochs
2. CSV file (.csv) containing the numerical values displayed in the loss graph
3. PyTorch model file (.pt) storing the trained model
4. PyTorch losses file (.pt) containing loss information
  
## Setup

1. Clone the git repository.
```git pull https://github.com/waltercywong/Congestion-Prediction-with-Walks-and-Paritioning.git```

3. Download the data titled `superblue.zip` from [here](https://zenodo.org/records/14599896) and place
the unzipped file in the `data` folder.

4. Install Python (3.9) and installing dependencies by running `pip install -r requirements.txt`

5. Place all the files in the `scripts` folder in the root directory of the 
DEHNN repository. These scripts can be run to perform analysis, create 
visualizations, or modify the processed data for the graph representation.

6. Replace the `train_all_cross.py` file in the `de_hnn` folder and 
`pyg_dataset.py` file in the `de_hnn\data` folder with those of the same name 
in the `modified` folder.

7. Run the notebook, analysis and model training scripts with the DEHNN 
environment.

