# Congestion Prediction in Chip Design with Walks and Paritioning

Chip design complexity has grown exponentially, leading to significant challenges in predicting and reducing wire congestion. Congestion occurs when excessive wire routing is concentrated in a constrained physical area, which degrades performance by causing overheating, signal interference, and excessive power usage.

This project explores congestion prediction in chip design by utilizing graph-based learning and partitioning techniques. We extend the work of the DE-HNN model by introducing dummy nodes to enhance long-range message passing and applying domain-based partitioning. These modifications aim to refine congestion-aware design decisions, optimizing chip layouts for better efficiency and performance. 

The repository includes notebooks and Python scripts for data analysis, model training, experimentation, and results of our tests. We evaluate these approaches against the original DE-HNN model, demonstrating their potential to improve congestion prediction in the early stages of chip design.

- Original DE-HNN repository: [Github](https://github.com/TILOS-AI-Institute/DEHNN) 
- Paper: [arXiv](https://arxiv.org/pdf/2404.00477) (Luo et al. (2024))

## File Structure
```
|
├───data
├───results
│   ├───BalancedKMeans
│   ├───Baseline
│   ├───Connections
│   ├───KMeans
│   ├───Louvain
│   ├───RandomWalk
│   ├───WeightedWalk
│   ├───XGBoost
│   └───XGBoostLimited
├───src
│   ├───models
│   ├───notebooks
│   ├───config.json
│   ├───pyg_dataset.py
│   ├───train_all_cross.py
│   ├───utils.py
│   └───run_all_data.py
├───scripts
│   ├───add_valid_connections.py
│   ├───analyze_graph_modularity.py
│   ├───analyze_pairs_detailed.py
│   ├───analyze_pairs.py
│   ├───analyze_paths.py
│   ├───louvain_creation.py
│   ├───partition_balanced_feature_approx_physical.py
│   ├───partition_feature_approx_physical.py
│   ├───partition_with_kahypar.py
│   ├───random_walk_rep.py
│   ├───valid_pairs_limited.py
│   ├───valid_pairs_test.py
│   ├───valid_pairs_weighted.py
│   ├───valid_pairs_xgb.py
│   └───valid_pairs.py
├───README.md
└───requirements.txt
```
## File Explanation

* `data`: contains all data from [here](https://zenodo.org/records/14599896) and generated by data from the scripts.


* `scripts`: contains all the scripts created for our experimentations.

   * `add_valid_connections.py`: Adds valid connections to the dataset.
   * `analyze_graph_modularity.py`: Analyzes the modularity of the graph.
   * `analyze_pairs_detailed.py`: Performs in-depth analysis on valid_pairs.csv.
   * `analyze_pairs.py`: Performs surface-level analysis on valid_pairs.csv.
   * `analyze_paths.py`: Analyzes paths in the graph (e.g., shortest paths or random walks).
   * `louvain_creation.py`: Implements the Louvain method for community detection.
   * `partition_balanced_feature_approx_physical.py`: Partitions the graph while balancing features and approximating physical constraints.
   * `partition_feature_approx_physical.py`: Partitions the graph while approximating physical constraints based on features.
   * `partition_with_kahypar.py`: Partitions the graph using the KaHyPar tool.
   * `random_walk_rep.py`: Modifies pyg_data.pkl to connect source-destination pairs based on random walk results.
   * `valid_pairs_limited.py`: Generates or processes a limited set of valid pairs.
   * `valid_pairs_test.py`: Tests the functionality of the valid_pairs.py script.
   * `valid_pairs_weighted.py`: Generates or processes valid pairs with weighted edges.
   * `valid_pairs_xgb.py`: Uses XGBoost to generate or analyze valid pairs.
   * `valid_pairs.py`: Generates the valid_pairs.csv file.

* `README.md`: Documentation file providing an overview of the repository and instructions for use.
* `requirements.txt`: Lists all Python dependencies required to run the scripts and notebooks.

* `results`:
   * `Baseline`: contains DEHNN baseline model implementation results
   * `Louvain`: contains Louvain-based model implementation results
   * `RandomWalk`: contains RandomWalk model implementation results
   * `Connections`: contains Connections model implementation results
   * `KMeans`: contains KMeans clustering model implementation results
   * `BalancedKMeans:` contains Balanced KMeans model implementation results
   * `XGBoost`: contains XGBoost model implementation results
   * `WeightedWalk`: contains Weighted Walk model implementation results
   * `XGBoostLimited`: contains Limited XGBoost model implementation results

Each implementation in the `results` folder contains:

1. Training loss visualization (.png) showing RMSE for train, validation, and test results over 100 epochs.
2. CSV file (.csv) containing the numerical values displayed in the loss graph.
3. PyTorch model file (.pt) storing the trained model.
4. PyTorch losses file (.pt) containing loss information.
  
## Setup

1. Clone the git repository.
```git pull https://github.com/waltercywong/Congestion-Prediction-with-Walks-and-Paritioning.git```

3. Download the data titled `superblue.zip` from [here](https://zenodo.org/records/14599896) and place
the unzipped file in the `data` folder.

4. Install Python (3.9) and dependencies by running `pip install -r requirements.txt`

5. Locate to the "/src" directory and run the following code to process the data.
```python run_all_data.py```


. Run the notebook, analysis and model training scripts with the DEHNN 
environment.

## Running Each Experiment

Every experiment we have conducted comes with its own requirements, in the following section, we will instruct how to run each of our tests.

### 1. 

