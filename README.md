# Congestion Prediction in Chip Design with Walks and Paritioning

Chip design complexity has grown exponentially, leading to significant challenges in predicting and reducing wire congestion. Congestion occurs when excessive wire routing is concentrated in a constrained physical area, which degrades performance by causing overheating, signal interference, and excessive power usage.

This project explores congestion prediction in chip design by utilizing graph-based learning and partitioning techniques. We extend the work of the DE-HNN model by introducing dummy nodes to enhance long-range message passing and applying domain-based partitioning. These modifications aim to refine congestion-aware design decisions, optimizing chip layouts for better efficiency and performance. 

The repository includes notebooks and Python scripts for data analysis, model training, experimentation, and results of our tests. We evaluate these approaches against the original DE-HNN model, demonstrating their potential to improve congestion prediction in the early stages of chip design.

- Original DE-HNN repository: [Github](https://github.com/TILOS-AI-Institute/DEHNN) 
- Paper: [arXiv](https://arxiv.org/pdf/2404.00477) (Luo et al. (2024))

## File Structure
```
|
├───data
├───src
│   ├───models
│   ├───notebooks
│   ├───config.json
│   ├───pyg_dataset.py
│   ├───train_all_cross.py
│   ├───utils.py
│   └───run_all_data.py
├───scripts
│   ├───add_valid_connections.py
│   ├───analyze_graph_modularity.py
│   ├───analyze_pairs_detailed.py
│   ├───analyze_pairs.py
│   ├───analyze_paths.py
│   ├───louvain_creation.py
│   ├───partition_balanced_feature_approx_physical.py
│   ├───partition_feature_approx_physical.py
│   ├───partition_with_kahypar.py
│   ├───random_walk_rep.py
│   ├───valid_pairs_limited.py
│   ├───valid_pairs_test.py
│   ├───valid_pairs_weighted.py
│   ├───valid_pairs_xgb.py
│   ├───trained_xgb_model.pkl
│   └───valid_pairs.py
├───results
│   ├───BalancedKMeans
│   ├───Baseline
│   ├───Connections
│   ├───KMeans
│   ├───Louvain
│   ├───RandomWalk
│   ├───WeightedWalk
│   ├───XGBoost
│   └───XGBoostLimited
├───README.md
└───requirements.txt
```
## File Explanation

* `data`: contains all data from [here](https://zenodo.org/records/14599896) and generated by data from the scripts.


* `scripts`: contains all the scripts created for our experimentations.

   * `add_valid_connections.py`: Adds valid connections to the dataset.
   * `analyze_graph_modularity.py`: Analyzes the modularity of the graph.
   * `analyze_pairs_detailed.py`: Performs in-depth analysis on valid_pairs.csv.
   * `analyze_pairs.py`: Performs surface-level analysis on valid_pairs.csv.
   * `analyze_paths.py`: Analyzes paths in the graph (e.g., shortest paths or random walks).
   * `louvain_creation.py`: Implements the Louvain method for community detection.
   * `partition_balanced_feature_approx_physical.py`: Partitions the graph while balancing features and approximating physical constraints.
   * `partition_feature_approx_physical.py`: Partitions the graph while approximating physical constraints based on features.
   * `partition_with_kahypar.py`: Partitions the graph using the KaHyPar tool.
   * `random_walk_rep.py`: Modifies pyg_data.pkl to connect source-destination pairs based on random walk results.
   * `valid_pairs_limited.py`: Generates or processes a limited set of valid pairs.
   * `valid_pairs_test.py`: Tests the functionality of the valid_pairs.py script.
   * `valid_pairs_weighted.py`: Generates or processes valid pairs with weighted edges.
   * `valid_pairs_xgb.py`: Uses XGBoost to generate or analyze valid pairs.
   * `trained_xgb_model.pkl`: Contains the trained XGBoost model for predictions.
   * `valid_pairs.py`: Generates the valid_pairs.csv file.

* `README.md`: Documentation file providing an overview of the repository and instructions for use.
* `requirements.txt`: Lists all Python dependencies required to run the scripts and notebooks.

* `results`:
   * `Baseline`: contains DEHNN baseline model implementation results
   * `Louvain`: contains Louvain-based model implementation results
   * `RandomWalk`: contains RandomWalk model implementation results
   * `Connections`: contains Connections model implementation results
   * `KMeans`: contains KMeans clustering model implementation results
   * `BalancedKMeans:` contains Balanced KMeans model implementation results
   * `XGBoost`: contains XGBoost model implementation results
   * `WeightedWalk`: contains Weighted Walk model implementation results
   * `XGBoostLimited`: contains Limited XGBoost model implementation results

Each implementation in the `results` folder contains:

1. Training loss visualization (.png) showing RMSE for train, validation, and test results over 100 epochs.
2. CSV file (.csv) containing the numerical values displayed in the loss graph.
3. PyTorch model file (.pt) storing the trained model.
4. PyTorch losses file (.pt) containing loss information.
  
## Setup

1. Clone the git repository.
```git pull https://github.com/waltercywong/Congestion-Prediction-with-Walks-and-Paritioning.git```

3. Download the data titled `superblue.zip` from [here](https://zenodo.org/records/14599896) and place
the unzipped file in the `data` folder.

4. Install Python (3.9) and dependencies by running `pip install -r requirements.txt`

5. Locate to the "/src" directory and run the following code to process the data.
```python run_all_data.py```



## Running Each Experiment

Every experiment we have conducted comes with its own requirements. Below are detailed instructions on how to run each of our tests. Ensure that the `config.json` file is properly configured for each experiment before running the training script (`train_all_cross_run.py`).

---

### 1. **Baseline**
- **Description**: This is the default experiment using the original configuration.
- **Steps**:
  1. Ensure `config.json` is set to the original configuration:
     ```json
     {
       "method_type": "original",
       "load_pe": true,
       "load_pd": true,
       "num_eigen": 10,
       "pl": true,
       "processed": true,
       "density": false
     }
     ```
  2. Run the training script:
     ```
     python train_all_cross_run.py
     ```

---

### 2. **Louvain**
- **Description**: This experiment uses the Louvain method for community detection.
- **Steps**:
  1. Run the Louvain creation script:
     ```
     python louvain_creation.py
     ```
  2. Update `config.json` to set the method type to `louvain`:
     ```json
     {
       "method_type": "louvain",
       "load_pe": true,
       "load_pd": true,
       "num_eigen": 10,
       "pl": true,
       "processed": true,
       "density": false
     }
     ```
  3. Run the training script:
     ```
     python train_all_cross_run.py
     ```

---

### 3. **RandomWalk**
- **Description**: This experiment uses random walks to generate graph representations.
- **Steps**:
  1. Run the random walk representation script:
     ```
     python random_walk_rep.py
     ```
  2. Update `config.json` to set the method type to `random_walk`:
     ```json
     {
       "method_type": "random_walk",
       "load_pe": true,
       "load_pd": true,
       "num_eigen": 10,
       "pl": true,
       "processed": true,
       "density": false
     }
     ```
  3. Run the training script:
     ```
     python train_all_cross_run.py
     ```

---

### 4. **Connections**
- **Description**: This experiment focuses on generating and validating connections between nodes.
- **Steps**:
  1. Generate valid pairs:
     ```
     python valid_pairs.py
     ```
  2. Test the valid pairs:
     ```
     python valid_pairs_test.py
     ```
  3. Add valid connections to the dataset:
     ```
     python add_valid_connections.py
     ```
  4. Update `config.json` to set the method type to `connection`:
     ```json
     {
       "method_type": "connection",
       "load_pe": true,
       "load_pd": true,
       "num_eigen": 10,
       "pl": true,
       "processed": true,
       "density": false
     }
     ```
  5. Run the training script:
     ```
     python train_all_cross_run.py
     ```

---

### 5. **KMeans**
- **Description**: This experiment uses KMeans clustering for partitioning the graph.
- **Steps**:
  1. Run the balanced feature approximation script:
     ```
     python partition_balanced_feature_approx_physical.py
     ```
  2. Update `config.json` to set the method type to `kmeans`:
     ```json
     {
       "method_type": "kmeans",
       "load_pe": true,
       "load_pd": true,
       "num_eigen": 10,
       "pl": true,
       "processed": true,
       "density": false
     }
     ```
  3. Run the training script:
     ```
     python train_all_cross_run.py
     ```

---

### 6. **BalancedKMeans**
- **Description**: This experiment uses a balanced version of KMeans clustering.
- **Steps**:
  1. Run the feature approximation script:
     ```
     python partition_feature_approx_physical.py
     ```
  2. Update `config.json` to set the method type to `kmeans`:
     ```json
     {
       "method_type": "kmeans",
       "load_pe": true,
       "load_pd": true,
       "num_eigen": 10,
       "pl": true,
       "processed": true,
       "density": false
     }
     ```
  3. Run the training script:
     ```
     python train_all_cross_run.py
     ```

---

### 7. **XGBoost**
- **Description**: This experiment uses XGBoost for generating or analyzing valid pairs.
- **Steps**:
  1. Run the XGBoost script:
     ```
     python valid_pairs_xgb.py
     ```
  2. Update `config.json` to set the method type to `xgb`:
     ```json
     {
       "method_type": "xgb",
       "load_pe": true,
       "load_pd": true,
       "num_eigen": 10,
       "pl": true,
       "processed": true,
       "density": false
     }
     ```
  3. Run the training script:
     ```
     python train_all_cross_run.py
     ```

---

### 8. **Weighted Walk**
- **Description**: This experiment uses weighted random walks for graph analysis.
- **Steps**:
  1. Analyze paths in the graph:
     ```
     python analyze_paths.py
     ```
  2. Generate weighted valid pairs:
     ```
     python valid_pairs_weighted.py
     ```
  3. Update `config.json` to set the method type to `weighted`:
     ```json
     {
       "method_type": "weighted",
       "load_pe": true,
       "load_pd": true,
       "num_eigen": 10,
       "pl": true,
       "processed": true,
       "density": false
     }
     ```
  4. Run the training script:
     ```
     python train_all_cross_run.py
     ```

---

### 9. **XGBoostLimited**
- **Description**: This experiment uses a limited version of XGBoost for generating valid pairs.
- **Steps**:
  1. Run the limited XGBoost script:
     ```
     python valid_pairs_limited.py
     ```
  2. Update `config.json` to set the method type to `xgb_lim`:
     ```json
     {
       "method_type": "xgb_lim",
       "load_pe": true,
       "load_pd": true,
       "num_eigen": 10,
       "pl": true,
       "processed": true,
       "density": false
     }
     ```
  3. Run the training script:
     ```
     python train_all_cross_run.py
     ```

Results will be saved in the same folder (`/src`)

---

### **Config File (`config.json`)**
The `config.json` file is central to all experiments. Ensure it is updated with the correct `method_type` and other parameters before running `train_all_cross_run.py`. Here is the default configuration:
```json
{
  "method_type": "original",
  "load_pe": true,
  "load_pd": true,
  "num_eigen": 10,
  "pl": true,
  "processed": true,
  "density": false
}
```

---

Thank you for viewing and running our project!

